"""ONNX æ¨¡å‹é‡åŒ– + éªŒè¯è„šæœ¬

æ”¯æŒå¤šç§é‡åŒ–æ–¹å¼:
1. FP16 - åŠç²¾åº¦æµ®ç‚¹ (æ¨è, å…¼å®¹æ€§æœ€å¥½)
2. åŠ¨æ€INT8 - ä»…é‡åŒ–MatMulå±‚ (è·³è¿‡Conv)
3. åŸå§‹FP32 - ä¸é‡åŒ–
"""

import os
import onnx
import onnxruntime as ort
import numpy as np
import cv2

# ================== é…ç½® ==================
INPUT_MODEL = "pose_simplified.onnx"   # åŸå§‹æ¨¡å‹
OUTPUT_FP16 = "pose_fp16.onnx"         # FP16æ¨¡å‹
OUTPUT_INT8 = "pose_int8.onnx"         # INT8æ¨¡å‹ (ä»…MatMul)
TEST_IMAGE = "test.png"                # æµ‹è¯•å›¾åƒ
IMGSZ = 416


def quantize_fp16():
    """FP16é‡åŒ– - å…¼å®¹æ€§æœ€å¥½"""
    from onnxconverter_common import float16

    print(f"\nğŸ”„ FP16é‡åŒ–: {INPUT_MODEL} -> {OUTPUT_FP16}")

    model = onnx.load(INPUT_MODEL)
    model_fp16 = float16.convert_float_to_float16(model, keep_io_types=True)
    onnx.save(model_fp16, OUTPUT_FP16)

    in_size = os.path.getsize(INPUT_MODEL) / 1024 / 1024
    out_size = os.path.getsize(OUTPUT_FP16) / 1024 / 1024
    print(f"âœ… å®Œæˆ! {in_size:.2f}MB -> {out_size:.2f}MB (å‹ç¼© {in_size/out_size:.1f}x)")
    return OUTPUT_FP16


def quantize_int8_safe():
    """INT8åŠ¨æ€é‡åŒ– - ä»…é‡åŒ–MatMulå±‚,è·³è¿‡Conv"""
    from onnxruntime.quantization import quantize_dynamic, QuantType

    print(f"\nğŸ”„ INT8é‡åŒ–(ä»…MatMul): {INPUT_MODEL} -> {OUTPUT_INT8}")

    quantize_dynamic(
        model_input=INPUT_MODEL,
        model_output=OUTPUT_INT8,
        weight_type=QuantType.QUInt8,  # ä½¿ç”¨QUInt8å…¼å®¹æ€§æ›´å¥½
        op_types_to_quantize=['MatMul', 'Gemm'],  # åªé‡åŒ–å…¨è¿æ¥å±‚,è·³è¿‡Conv
    )

    in_size = os.path.getsize(INPUT_MODEL) / 1024 / 1024
    out_size = os.path.getsize(OUTPUT_INT8) / 1024 / 1024
    print(f"âœ… å®Œæˆ! {in_size:.2f}MB -> {out_size:.2f}MB (å‹ç¼© {in_size/out_size:.1f}x)")
    return OUTPUT_INT8


def validate(model_path):
    """éªŒè¯æ¨¡å‹æ¨ç†"""
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")

    try:
        session = ort.InferenceSession(
            model_path,
            providers=['CPUExecutionProvider']
        )
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return False

    # è·å–è¾“å…¥ä¿¡æ¯
    input_info = session.get_inputs()[0]
    input_name = input_info.name
    print(f"   è¾“å…¥: {input_name} {input_info.shape}")

    # è·å–è¾“å‡ºä¿¡æ¯
    for out in session.get_outputs():
        print(f"   è¾“å‡º: {out.name} {out.shape}")

    # åŠ è½½æµ‹è¯•å›¾åƒ
    if os.path.exists(TEST_IMAGE):
        print(f"ğŸ–¼ï¸  æµ‹è¯•å›¾åƒ: {TEST_IMAGE}")
        img = cv2.imread(TEST_IMAGE)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (IMGSZ, IMGSZ))
    else:
        print(f"ğŸ² ä½¿ç”¨éšæœºè¾“å…¥ (æœªæ‰¾åˆ° {TEST_IMAGE})")
        img = np.random.randint(0, 255, (IMGSZ, IMGSZ, 3), dtype=np.uint8)

    # é¢„å¤„ç†
    img = img.astype(np.float32) / 255.0
    img = img.transpose(2, 0, 1)[None]  # (1, 3, H, W)

    # æ¨ç†
    try:
        outputs = session.run(None, {input_name: img})
        print(f"âœ… æ¨ç†æˆåŠŸ!")
        print(f"   è¾“å‡ºå½¢çŠ¶: {[o.shape for o in outputs]}")
        return True
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        return False


def benchmark(model_path, runs=50):
    """æ€§èƒ½æµ‹è¯•"""
    import time

    try:
        session = ort.InferenceSession(
            model_path,
            providers=['CPUExecutionProvider']
        )
    except Exception as e:
        print(f"âš ï¸  {os.path.basename(model_path)}: æ— æ³•åŠ è½½ - {e}")
        return None

    input_name = session.get_inputs()[0].name
    dummy = np.random.randn(1, 3, IMGSZ, IMGSZ).astype(np.float32)

    # é¢„çƒ­
    for _ in range(5):
        session.run(None, {input_name: dummy})

    # æµ‹é€Ÿ
    t0 = time.time()
    for _ in range(runs):
        session.run(None, {input_name: dummy})
    ms = (time.time() - t0) / runs * 1000

    name = os.path.basename(model_path)
    size = os.path.getsize(model_path) / 1024 / 1024
    print(f"â±ï¸  {name}: {ms:.2f}ms ({1000/ms:.1f} FPS) | {size:.2f}MB")
    return ms


def main():
    if not os.path.exists(INPUT_MODEL):
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ¨¡å‹: {INPUT_MODEL}")
        return

    print("=" * 50)
    print("ONNX æ¨¡å‹é‡åŒ–å·¥å…·")
    print("=" * 50)

    # éªŒè¯åŸå§‹æ¨¡å‹
    print("\n[1/4] éªŒè¯åŸå§‹æ¨¡å‹")
    if not validate(INPUT_MODEL):
        print("åŸå§‹æ¨¡å‹æ— æ³•è¿è¡Œ,è¯·æ£€æŸ¥")
        return

    # FP16é‡åŒ– (æ¨è)
    print("\n[2/4] FP16é‡åŒ–")
    try:
        fp16_model = quantize_fp16()
        fp16_ok = validate(fp16_model)
    except ImportError:
        print("âš ï¸  éœ€è¦å®‰è£… onnxconverter-common: pip install onnxconverter-common")
        fp16_ok = False
    except Exception as e:
        print(f"âš ï¸  FP16é‡åŒ–å¤±è´¥: {e}")
        fp16_ok = False

    # INT8é‡åŒ– (ä»…MatMul)
    print("\n[3/4] INT8é‡åŒ–(å®‰å…¨æ¨¡å¼)")
    try:
        int8_model = quantize_int8_safe()
        int8_ok = validate(int8_model)
    except Exception as e:
        print(f"âš ï¸  INT8é‡åŒ–å¤±è´¥: {e}")
        int8_ok = False

    # æ€§èƒ½å¯¹æ¯”
    print("\n[4/4] æ€§èƒ½å¯¹æ¯”")
    print("-" * 50)
    benchmark(INPUT_MODEL)
    if fp16_ok:
        benchmark(OUTPUT_FP16)
    if int8_ok:
        benchmark(OUTPUT_INT8)

    # æ€»ç»“
    print("\n" + "=" * 50)
    print("æ¨èä½¿ç”¨:")
    if fp16_ok:
        print(f"  âœ… {OUTPUT_FP16} (FP16, å…¼å®¹æ€§å¥½, çº¦50%å‹ç¼©)")
    if int8_ok:
        print(f"  âœ… {OUTPUT_INT8} (INT8éƒ¨åˆ†é‡åŒ–)")
    print(f"  âœ… {INPUT_MODEL} (åŸå§‹FP32, æœ€é«˜ç²¾åº¦)")


if __name__ == "__main__":
    main()
